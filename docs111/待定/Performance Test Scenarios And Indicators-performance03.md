---
title: Performance Test Scenarios And Indicators
date: 2022-09-04 15:53:34.384
updated: 2022-09-15 07:50:52.975
url: /archives/performance03
categories: 
- jmeter
tags: 
- jmeter
---

# 性能测试场景及指标
## 本章要点
- [ ] 性能测试场景
- [ ] 性能测试指标


## 性能测试时间节点
上一小节介绍了对应性能测试的分类，那在什么去进行性能测试呢？在什么时候给领导提出来，系统需要做性能测试呢？可以通过一下几个阶段考虑。
### 性能验收 A
- 业务系统对性能有明确的要求

有了指标，就可以很容易的进行测试，而不是瞎子摸象那种，需要自己去找指标

>性能测试
### 性能评估 B
- 需要评估业务系统的性能效率
>负载测试

### 功能缺陷发现
- 验证在高并发下，系统业务是否有问题
>压力测试

### 技术缺陷发现/性能调优
- 验证在高并发下，技术架构是否有问题 - 技术缺陷发现/性能调优
>压力测试

### 稳定性缺陷发现

- 验证在高并发下，系统是否可靠运行

>稳定性测试
### 基准对照 C

- 验证在系统变更后，系统性能表现变化情况
>基准测试



**性能测试针对系统的性能指标，建立性能测试模型，制定性能测试方案，制定监控策略，在场景条件之下执行性能场景，分析判断性能瓶颈并调优，最终得出性能结果来评估系统的性能指标是否满足既定值。**

## 性能测试技术分层
对于性能测试来说，性能测试技术可以分为三个层次：**用户行为模拟**、**指标监控**、**性能调优**

## 用户行为模拟
### 概念
性能测试是需要通过虚拟用户的方式去模拟用户的行为，即去模拟用户请求服务器的行为。
### 分类
对于用户行为模拟来说，分为两个部分：**脚本**、**场景**。
####  脚本
####  场景
### 性能测试脚本
对于脚本来说，是**模拟负载用户发送请求的数据包**。

#### 负载用户
性能测试是需要去模拟大量的 **虚拟用户** {*负载用户*} 访问服务器。
##### 分类
负载用户参考标准是什么？ ` {参考值的来源}`

一般，负载用户的分类有三种：
###### 系统最大用户数
- 指使用系统的最大人数。

>最大用户数的用户，并不一定是真正的自然人；只要注册、登录了就算。

###### 在线用户数
在线：指长时间、大量访问系统的用户。
某段时间内访问系统的用户数，这些用户只是在线。不一定同时做某一件事情。
- **在线** 比 **系统最大用户数** 要 `小`
###### 并发用户数
- 性能并发：指同时向服务器发起请求的用户数。或者对于服务器而言，同时正在处理的请求数（连接）。
- 虽然都是向服务器提交请求，但是场景不一定是同一个
>通用含义：在线即并发；产品或研发理解的并发，无需普及该知识给产品，最终报告结果告知是否满足需求即可。
>性能工程师，要会把客户或产品需求转换为专业知识。
		
##### 注意⚠️
- 一般，开发和客户所谓的并发量都是指 在线的并发 。
-  性能指标中，习惯以 **并发量** 大概是 **在线用户数** 的 `5%-20%` 左右。

>一个OA系统，该系统有2000个使用用户；
>使用该OA系统的用户总数是2000名，即 “系统用户数” 为2000，
>系统有一个“在线统计”功能「系统用一个全局变量记数所有已登录的用户」，从在线统计功能中可以得到，最高峰时有500人在线（即“同时在线人数”为500）
>500的5%-20%为25-100。
>根据业务场景预估，500人中40%的用户在较有兴致地看系统公告「`看`不会给服务器造成压力」，40%的用户在填写页面表单但是未进行提交「`填写`对服务器不构成压力，`提交`的时刻才会向服务器发送请求」，可能只有20% 即 100个人会不停地从一个页面跳转到另一个页面，并发用户数就可以假设为100个进行设置。

从上面的例子中可以看出，服务器实际承受的压力不只取决于业务并发用户数，还取决于用户的业务场景。只是登录，但是用户并没有对服务发起请求，即无压力发送给服务端，则不算并发用户。

问题：
一个线程100次，和一百个线程一次，有什么区别？？
![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209041437830.png)
虽然最后压测结果PV都是100，但是这2者区别很大


##### 需求
- 一个系统，需要在一天的时间内支撑100W的PV「Page View」

###### 说明
1. 100WPV不等于100W的并发数。
2. 在没有更多的数据参考下，可以使用 2/8 模型
- 80%的业务发生在20%的时间内
###### TPS
预期的`TPS` = （100W * 0.8）/（24 * 3600 * 0.2） = 46.3




- 负载用户 - 在线用户数：长时间、大量访问系统的用户
- 一般，**并发量** 大概是 **在线用户数** 的 `5%-20%` 左右






要更真实的去模拟用户行为，我们一般要考虑做到以下操作：

#### 参数化

模拟不同用户的不同行为。即逻辑相同，并发的数据不同的效果。

#### 关联

处理请求之间的依赖。

#### 思考时间、集合点

模拟用户的延迟和并发。



### 性能测试场景

脚本本身只是单个用户的行为，而性能测试是需要考虑多个用户向服务器发起请求的情况，那么我们就需要去考虑性能测试场景的设计。**脚本最终是为了场景而服务的**。

一般来说，性能测试场景越真实越好。


#### 业务理解分类

通常来说，性能测试场景根据业务理解分类分为两种：

##### 业务场景

通常指的是系统的业务处理流程，描述具体的用户行为，通过对用户行为进行分析，以划分出不同的业务场景，是性能测试时测试场景设计的重要来源

>性能测试肯定是对应的公司内很重要的业务才会进行性能优化测试，不是说把公司所有的业务去进行这个性能压测。 所以，在做性能测试之前，要熟悉明白功能测试。

**功能测试是一切测试的基础**

###### 性能测试不知道测试什么？

重要的业务+有负载需求的业务

**保本原则**：针对主要的业务「看需求量最重要的部分」场景进行性能测试

>OA系统的审批，只有一个管理员；该功能很重要，但是没有负载需求，最多就是批量审批，但是也不会说批量审批上万个审批单。
>但是，多个员工提交审批单，这一个接口是需要做性能测试的。有上千个上万个员工，同时提交审批单，这个是一个高并发的请求，有可能大家排队提交都提交不上去，需要等待重试。
>性能测试不是为了测而测试，需要首先了解系统的业务场景，再根据场景内的内容去找到高负载的请求，进而进行压力测试。
这个业务有多少人用、有多少人同时用、多少人同时用的时候做的是什么操作？等等，这就叫 **业务建模**

##### 测试场景
测试场景是对业务场景的真实模拟，测试场景的设计应该尽可能贴近真实的业务场景，有时候由于测试条件的限制，可以适当做一些调整和特殊的设置等
>测试场景就是开发脚本
- 测试场景包含：单场景、混合场景

###### 单场景

- 一个场景中只执行一种业务

指的是只涉及单个业务流程的测试场景，目的是测试系统的单个业务处理能力是否达到预期，并且得到系统资源利用正常情况下的最大TPS，平均响应时间等性能指标

- 举例：登录、查询、支付这样单独的是一个单场景
###### 混合场景
- 一个场景中执行多种业务

测试场景中涉及多个业务场景，并且 **每个业务流程在混合的业务流程中占的比重会不同** ，该比重一般根据实际的业务流程来设定，尽可能符合实际的业务需求。该测试场景的目的是为了测试系统的混合业务处理能力是否满足预期要求，并且评估系统的混合业务处理容量最大能达到多少

>一个商城系统，不可能只有一个用户，并且一个用户只有一个行为。比如，有多个用户，有一部分人登录、有一部分人退出、有一部分人查询、有一部分人下订单、还有一部分人退款，等等，这就是一个混合场景。
- **混合场景** 是由 **单场景** 构成的。

一般公司，第一个版本都不会进行性能测试，如果进行，也不是为了测试当前系统的一个性能负载，而是为了看一下系统是否有重大的内存泄露或者死锁等问题。

然后线上试运行的时候研发进行数据的收集，在一天中最大同时在线用户数，各个重要功能业务的并发数，然后反馈给性能测试人员，再进行混合业务场景的压力测试。

#### 按照过程分类
“过程”，说的其实就是我们应该怎样执行性能场景、性能场景应该从哪里开始的问题。
![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209051807603.webp)


##### 基准性能场景
基准性能场景：这里要做的是单交易的容量，为混合容量做准备（不要跟我说上几个线程跑三五遍脚本叫基准测试，在我看来，那只是场景执行之前的预执行，用来确定有没有基本的脚本和场景设计问题，不能称之为一个分类）。
##### 容量性能场景
容量性能场景：这一环节必然是最核心的性能执行部分。根据业务复杂度的不同，这部分的场景会设计出很多个，在说完对应的性能指标后，进行具体的场景设计。
##### 稳定性性能场景
稳定性性能场景：稳定性测试必然是性能场景的一个分类。只是现在在实际的项目中，稳定性测试基本没和生产一致过。在稳定性测试中，显然最核心的元素是时间（业务模型已经在容量场景中确定了），而时间的设置应该来自于运维周期，而不是来自于老板、产品和架构等这些人的心理安全感。
##### 异常性能场景
异常性能场景：要做异常性能场景，前提就是要有压力。在压力流量之下，模拟异常。这个异常的定义是很宽泛的。













## 指标监控

常见的性能指标有以下几种：
- 响应时间  RT
- 吞吐量
- 资源利用率


### 响应时间  RT
Response Time
性能本质 中 **系统的处理效率** 的指标。一般统计的是：事务处理的响应时间。
- 响应时间的概念简单至极，但是，响应时间的定位就复杂了。


从发出一个请求到获取请求响应的时间；
>地址栏输入百度网址回车后，到页面展示百度网址的内容，这就是响应时间。
>
>在性能指标中，关注的点不是在地址栏输入网址的时间快慢，也不是按下回车的快慢。而是按完回车后，对应页面展示内容的时间的快慢，即百度服务器处理该请求的快慢。

响应时间不需要自己收集，性能工具会自动帮我们收集并展示。

#### 包括
- 用户端呈现数据的时间
- 请求/响应数据 网络传输时间啊
- 应用服务器处理时间
- 数据库系统处理时间

![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209150007601.png)
>一次http请求经过的路径，请求会经过网络发送到web服务器进行处理，如果需要操作DB，再由网络转发到数据库进行处理，然后返回值给web服务器，web服务器最后把结果数据通过网络返回给客户端
#### 响应时间参考
多少合理？
- 1.同行业对比数据
- 2.找系统样本用户，统计，评估用户对响应时间满意度。
### 吞吐量
- 用来反映系统的处理能力的指标。
- 反应的是服务器承受的压力。
- 吞吐量能够说明系统的负载能力。
>以下，都是用来反映系统处理的能力指标，只不过是统计的维度有区别。
>比如说，今天营业额是多少「throughput」，卖了多少件商品「tps」

- 软件系统在每单位时间内能处理多少个事务/请求/单位数据等的量。

#### 表示
吞吐量 用 请求数/秒 、 页面数/秒、 人数/天 或 处理业务数/小时 等单位来衡量。
#### 注意
一般来说，响应时间和吞吐量指标不需要我们做额外的处理，通常会由性能测试工具本身来收集和整理。

#### TPS
TPS 是性能领域中一个关键的性能指标概念，**它用来描述每秒事务数**。
我们也知道 TPS 在不同的行业、不同的业务中定义的粒度都是不同的。
所以不管你在哪里用 TPS，一定要有一个前提，就是所有相关的人都要知道你的 **T** 是如何定义的。
 
 通常情况下，我们会根据场景的目的来定义 TPS 的粒度。
 - 如果是 **接口层性能测试**，T 可以直接定义为 **接口级**；
 - 如果 **业务级性能测试**，T 可以直接定义为 **每个业务步骤和完整的业务流**。「事务级」
 
 ![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209061819264.webp)
 如果我们要单独测试接口 1、2、3，那 T 就是接口级的；
 如果我们要从用户的角度来下一个订单，那 1、2、3 应该在一个 T 中，这就是业务级的了。
 当然，这时我们还要分析系统是如何设计的。
 
 通常情况下，**积分** 我们都会 **异步**，而 **库存** *不* 能 *异步* 。
 所以这个业务，你可以看成只有 1、2 两个接口，但是在做这样的业务级压力时，3 接口也是必须要监控分析的。所以，性能中 TPS 中 T 的定义取决于场景的目标和 T 的作用。一般我们都会这样来定事务。
 
##### 接口级脚本：
 ——事务 start（接口 1）
 接口 1 脚本
 ——事务 end（接口 1）
 ——事务 start（接口 2）
 接口 2 脚本
 ——事务 end（接口 2）
 ——事务 start（接口 3）
 接口 3 脚本
 ——事务 end（接口 3）
 
##### 业务级接口层脚本（就是用接口拼接出一个完整的业务流）：
 ——事务 start（业务 A）
 接口 1 脚本 - 接口 2（同步调用）
 接口 1 脚本 - 接口 3（异步调用）
 ——事务 end（业务 A）
 
 ##### 用户级脚本
 ——事务 start（业务 A）
 点击 0 - 接口 1 脚本 - 接口 2（同步调用）
 点击 0 - 接口 1 脚本 - 接口 3（异步调用）
 ——事务 end（业务 A）
 
 >你要创建什么级别的事务，完全取决于测试的目的是什么。一般情况下，我们会按从上到下的顺序一一地来测试，这样路径清晰地执行是容易定位问题的。
 
#### tps/qps/hps/rps
>`ps` ：`pre second` 每秒
- `tps`
 `t`来源于`LoadRunner` 「每秒事务数」 Transactions Per Second
 系统每秒能够处理的交易和事务数量，一般统计的是每秒处理的事务数
 
- `qps`
 `q` 来源互联网， `query` 「每秒请求数」
 QPS 一开始是用来描述 MySQL 中 SQL 每秒执行数 Query Per Second，所有的 SQL 都被称为 Query。
 后来，由于一些文章的转来转去，QPS 被慢慢地移到了压力工具中，用来描述吞吐量，于是这里就有些误解，QPS 和 TPS 到底是什么关系呢？
 
- `hps`「点击率」
  `hits pre second` 
  反映客户每秒向服务端提交的请求数。
  性能测试中，一般不发起静态请求，所以 `hit` 通常是指的**动态请求**。
> 从协议的角度而言，`qps`、`hps` 、`rps` 是等价的
>`tps`是我们自定义的。 一个事务里面可以有一个请求`qps`可以有三个四个请求`qps`，所以，`tps`是不等于`qps`的；
>如果一个事务里面只有一个请求，这一个请求只会发起一个协议层面的连接，则 `tps`、`qps`、`hps`三者等价。


- `rps`
 RPS 指的是每秒请求数。这个概念字面意思倒是容易理解，但是有个容易误解的地方就是，它指的到底是哪个层面的 Request？
 如果说 HTTP Request，那么和 Hits Per Second 又有什么关系呢？
#### PV/UV/MC
服务器统计指标：`page view`「页面访问量」，`user view`「访问用户」

前端考虑的性能指标：`mouse click`「点击量」
- `PV`
一般是衡量电子商务网站性能测试容量的重要指标。PV的统计可以发为全天- `PV`，每小时的`PV`以及峰值`PV`（高峰期1小时的`PV`）
- `UV`
 分为全天`UV`\每小时`UV`以及峰值`UV`(高峰1小时的`UV`)
 
 
#### throughput
常见于网络带宽、交互数据、磁盘IO等，单位都是byte（bit）。
>单位时间内会产生多少个数据量。
从不同角度，吞吐量的计算方式可以不一样：

*业务角度*：吞吐量可以用 **请求数/s** ，**页面数/s** 等来进行衡量计算

*网络角度*：吞吐量可以用 **字节/s** 来进行衡量计算

*应用角度*：吞吐量指标反映的是服务器承受的压力，即**系统的负载能力**


 ![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209150010008.jpeg)
 上图中蓝线表示TPS，黄色表示响应时间。

在TPS增加的过程中，响应时间一开始会处在较低的状态，也就是在A点之前。接着响应时间开始有些增加，直到业务可以承受的时间点B，这时 TPS 仍然有增长的空间。再接着增加压力，达到C点时，达到最大 TPS。我们再接着增加压力，响应时间接着增加，但 TPS 会有下降（请注意，这里并不是必然的，有些系统在队列上处理得很好，会保持稳定的 TPS ，然后多出来的请求都被友好拒绝）。

最后，响应时间过长，达到了超时的程度。


Response time = (N1+N2+N3+N4)+ (A1+A2+a3)，即：（网络时间 + 应用程序处理时间）

### 资源利用率
CPU、内存、磁盘IO、带宽等等

>一般来说，资源利用率作为性能指标的意义是远远小于其作为 **分析、定位瓶颈的支撑数据** 的意义。

- 资源利用率一般为 **分析、定位系统瓶颈的支撑数据** 

>资源的开销情况对性能指标有着重要的影响，一般做性能优化时，都是尽可能优化到让每一个请求或者事务对系统资源的消耗少到最小

一般来说，不建议通过集成监控的方式来实现对服务器资源的监控。
对于服务器资源的监控，建议直接在服务器上通过命令、第三方监控组件来实现监控。

 ![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209061813723.webp)




## 性能指标计算
### 压力工具中的线程数和用户数与 TPS
#### 线程数与 TPS
总是有很多人在并发线程数和 TPS 之间游荡，搞不清两者的关系与区别。这两个概念混淆的点就是，好像线程是真实的用户一样，那并发的线程是多少就描述出了多少真实的用户。


![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209061830220.webp)
前提：上面的一个框中有四个箭头，每个都代表着相同的事务。

在上面的内容中，我们说了好多的指标，但并发是需要具体的指标来承载的。你可以说，我的并发是 1000TPS，或者 1000RPS，或者 1000HPS，这都随便你去定义。但是在一个具体的项目中，当你说到 **并发1000**这样没有单位的词时，一定要让大家都能理解这是什么。


在上面这张示意图中，其实 压力工具 是 **4个并发线程**，由于每个线程都可以在 一秒内 完成 4 个事务 ，所以总的 **TPS** 是 **16**。
这非常容易理解吧。而在大部分非技术人的脑子里，这样的场景就是并发数是 4，而不是 16。


#### 总结
1. 并发度TPS 不等于线程数 
2. tps = 线程数 X 每个线程1s内完成T的次数


### 用户数
涉及到用户就会比较麻烦一点。因为用户有了业务含义，所以有些人认为一个系统如果有 1 万个用户在线，那就应该测试 1 万的并发线程，这种逻辑实在是不技术。
通常，我们会对在线的用户做并发度的分析，在很多业务中，并发度都会低于 5%，甚至低于 1%。

拿 5% 来计算，就是 10000 用户 x5%=500(用户级 TPS)，注意哦，这里是 TPS，而不是并发线程数。
如果这时响应时间是 100ms，那显然并发线程数是 500TPS/(1000ms/100ms)=50(并发线程)。
![](https://cdn.jsdelivr.net/gh/testeru-top/top-images/jmeter/202209061840086.webp)



但是！响应时间肯定不会一直都是 100ms 的嘛。所以通常情况下，上面的这个比例都不会固定，而是随着并发线程数的增加，会出现趋势上的关系。所以，在性能分析中，我一直在强调着一个词：趋势！





## 性能调优

性能调优不是性能测试工程师的工作，性能调优应该是整个项目团队的工作。

对于性能测试工程师而言，主要的职责是去实现性能测试场景，获取有效的测试数据，监控数据，并且对之进行分析，定位性能瓶颈。一旦定位到性能瓶颈，则由相关的责任人去实现性能调优。

## 性能测试误区
##### 误区1
性能测试 = 性能测试工具的使用
- 目标不明确、缺乏规划
	
    没有添加对应的场景分析，没有业务场景，也没有单场景和混合场景，什么都没有，上来就是使用工具进行性能测试。
>面试的时候，面试官问你是否会性能测试。不要回答，我会使用JMeter。
>如果这样回答，则你的潜意识认为性能测试就是工具的使用。


实际工具的使用只是性能测试的一部分，重要的还是拿到对应的结果数据，进行分析，给出研发或运维对应的调优建议。
##### 误区2
性能测试 = 功能测试 + 并发量
- 测试所有场景

>好处：不会遗漏功能点。坏处：时间长。
##### 误区3
性能测试 = GUI界面性能测试
- APP、Web、小程序多管齐下
>一般情况，10%的性能问题在前端；80%-90%的性能问题在后端「主要了解后端的性能接口测试」
##### 误区4
性能测试 = 性能脚本运行
- 缺乏数据收集、分析、调优过程
>每次脚本运行完，对应的数据收集、分析、调优后，才能进行下一步。

##### 误区5
实验室下性能指标 = 实际环境下性能指标
- 忽略了环境的差异

##### 误区6
采用功能测试的思维展开性能测试
- 性能测试通常采用实验的形式，迭代的进行。目的是为了支持系统的分析、设计和实施。







